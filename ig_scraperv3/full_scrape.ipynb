{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.50.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: pyee<13,>=12 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from playwright) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/chaewonbang/Library/Python/3.12/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/chaewonbang/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install playwright pandas python-dotenv\n",
    "!python3 -m playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/Resources/Python.app/Contents/MacOS/Python: can't open file '/Users/chaewonbang/your-story/ig-project/ig_scraperv3/install': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python3 install playwright pandas python-dotenv\n",
    "!playwright install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from playwright.async_api import async_playwright, TimeoutError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "username = os.getenv(\"USERNAME\")\n",
    "password = os.getenv(\"PASSWORD\")\n",
    "\n",
    "# Define login function\n",
    "async def signon(page, username, password):\n",
    "    try:\n",
    "        await page.goto(\"https://www.instagram.com/accounts/login/\", wait_until=\"networkidle\")\n",
    "        await page.wait_for_selector('input[name=\"username\"]', timeout=10000)\n",
    "        await page.fill('input[name=\"username\"]', username)\n",
    "        await page.fill('input[name=\"password\"]', password)\n",
    "        await page.wait_for_timeout(500)\n",
    "        print(\"Hitting login button\")\n",
    "\n",
    "        # Wait until the element with the aria-label \"Log in\" is visible.\n",
    "        await page.wait_for_selector(\"div[aria-label='Log in']\", state=\"visible\")\n",
    "        # Now attempt a click on it.\n",
    "        await page.click(\"div[aria-label='Log in']\")\n",
    "\n",
    "        #wait for networkidle\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "        \n",
    "        #look for this: <span class=\"x1lliihq x193iq5w x6ikm8r x10wlt62 xlyipyv xuxw1ft\">Save info</span>\n",
    "        # the button is its 5th parent\n",
    "        await page.wait_for_selector(\"span:has-text('Save info')\", timeout=15000)\n",
    "        # Click the button\n",
    "        await page.click(\"span:has-text('Save info')\", timeout=5000)\n",
    "        #wait for networkidle\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "        if \"login\" in page.url:\n",
    "            raise ValueError(\"Login failed: Incorrect username or password.\")\n",
    "        print(\"‚úÖ Successfully logged in\")\n",
    "    except TimeoutError:\n",
    "        raise TimeoutError(\"Login timed out. Check credentials or network connection.\")\n",
    "\n",
    "# Define helper to get total posts\n",
    "async def get_total_posts(page):\n",
    "    total_posts = await page.evaluate(\"\"\"\n",
    "        () => {\n",
    "            const element = document.querySelector('header section ul li span');\n",
    "            return element ? parseInt(element.innerText.replace(',', '')) : null;\n",
    "        }\n",
    "    \"\"\")\n",
    "    return total_posts if total_posts is not None else float('inf')\n",
    "\n",
    "# Modified scrape_instagram_posts to stop when a post is older than Jan 1, 2023\n",
    "async def scrape_instagram_posts(userhandle: str, max_posts: int, context, page):\n",
    "    profile_url = f\"https://www.instagram.com/{userhandle}/\"\n",
    "    await page.goto(profile_url)\n",
    "    await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "    total_posts = await get_total_posts(page)\n",
    "    scrape_limit = min(max_posts, total_posts)\n",
    "    print(f\"üîé {userhandle}: Total posts {total_posts}, scraping up to {scrape_limit}...\")\n",
    "\n",
    "    unique_posts = {}\n",
    "    scroll_attempts = 0\n",
    "    MAX_SCROLL_ATTEMPTS = 20\n",
    "\n",
    "    while len(unique_posts) < scrape_limit and scroll_attempts < MAX_SCROLL_ATTEMPTS:\n",
    "        candidate_elements = await page.query_selector_all(\"a:has(div._aagu)\")\n",
    "        for element in candidate_elements:\n",
    "            href = await element.get_attribute(\"href\")\n",
    "            if href and \"/p/\" in href and href not in unique_posts:\n",
    "                unique_posts[href] = None\n",
    "        print(f\"üîÑ Scrolled {scroll_attempts + 1}x ‚Äî Collected: {len(unique_posts)}\")\n",
    "        if len(unique_posts) >= scrape_limit:\n",
    "            break\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        await asyncio.sleep(1.5)\n",
    "        scroll_attempts += 1\n",
    "\n",
    "    post_hrefs = list(unique_posts.keys())[:scrape_limit]\n",
    "    results = {}\n",
    "\n",
    "    CUTOFF_DATE = datetime(2023, 1, 1)\n",
    "    print(f\"üìù Intercepting posts (cutoff: {CUTOFF_DATE.date()})...\")\n",
    "\n",
    "    for i, href in enumerate(post_hrefs, start=1):\n",
    "        post_url = f\"https://www.instagram.com{href}\"\n",
    "        new_page = await context.new_page()\n",
    "        try:\n",
    "            async with new_page.expect_response(\n",
    "                lambda response: \"/api/v1/media/\" in response.url and \"/info/\" in response.url,\n",
    "                timeout=5000\n",
    "            ) as response_info:\n",
    "                await new_page.goto(post_url)\n",
    "\n",
    "            response = await response_info.value\n",
    "            data = await response.json()\n",
    "\n",
    "            # Check the post timestamp\n",
    "            timestamp = data.get(\"items\", [{}])[0].get(\"taken_at\")\n",
    "            if timestamp:\n",
    "                post_date = datetime.fromtimestamp(timestamp)\n",
    "                if post_date < CUTOFF_DATE:\n",
    "                    print(f\"üõë Post {href} is from {post_date.date()}, before 2023. Stopping.\")\n",
    "                    await new_page.close()\n",
    "                    break\n",
    "\n",
    "            results[href] = data\n",
    "            print(f\"‚úÖ ({i}/{len(post_hrefs)}) {href} ‚Äî {post_date.date()}\")\n",
    "\n",
    "        except TimeoutError:\n",
    "            print(f\"‚è±Ô∏è Timeout: {href}\")\n",
    "        finally:\n",
    "            await new_page.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Batch scrape users from a CSV and save to JSON\n",
    "async def scrape_users_from_csv(csv_path: str, max_posts_per_user: int, output_json: str):\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    usernames = df[0].dropna().unique().tolist()\n",
    "\n",
    "    if os.path.exists(output_json):\n",
    "        with open(output_json, 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        device = p.devices[\"Pixel 5\"]\n",
    "        context = await browser.new_context(**device)\n",
    "        page = await context.new_page()\n",
    "\n",
    "        if os.path.exists(\"cookies.json\"):\n",
    "            print(\"üîÑ Loading cookies...\")\n",
    "            with open(\"cookies.json\", \"r\") as f:\n",
    "                cookies = json.load(f)\n",
    "            await context.add_cookies(cookies)\n",
    "        else:\n",
    "            print(\"üîê Logging in...\")\n",
    "            await signon(page, username, password)\n",
    "            cookies = await context.cookies()\n",
    "            with open(\"cookies.json\", \"w\") as f:\n",
    "                json.dump(cookies, f)\n",
    "\n",
    "        for user in usernames:\n",
    "            if user in all_results:\n",
    "                print(f\"‚è© Skipping {user} (already scraped)\")\n",
    "                continue\n",
    "            try:\n",
    "                result = await scrape_instagram_posts(user, max_posts_per_user, context, page)\n",
    "                all_results[user] = result\n",
    "                with open(output_json, 'w') as f:\n",
    "                    json.dump(all_results, f, indent=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error scraping {user}: {e}\")\n",
    "\n",
    "        await browser.close()\n",
    "        print(f\"\\n‚úÖ All scraping complete. Results saved to {output_json}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading cookies...\n",
      "üîé sooyaaa__: Total posts 1217, scraping up to 50...\n",
      "üîÑ Scrolled 1x ‚Äî Collected: 22\n",
      "üîÑ Scrolled 2x ‚Äî Collected: 34\n",
      "üîÑ Scrolled 3x ‚Äî Collected: 42\n",
      "üîÑ Scrolled 4x ‚Äî Collected: 54\n",
      "üìù Intercepting posts (cutoff: 2023-01-01)...\n",
      "‚úÖ (1/50) /sooyaaa__/p/DIaPpp9z7Wq/ ‚Äî 2025-04-13\n",
      "‚úÖ (2/50) /sooyaaa__/p/DIX8nCBpjYY/ ‚Äî 2025-04-12\n",
      "‚úÖ (3/50) /sooyaaa__/p/DIJGGzIp_M9/ ‚Äî 2025-04-07\n",
      "‚úÖ (4/50) /sooyaaa__/p/DH_OVJQpBsM/ ‚Äî 2025-04-03\n",
      "‚úÖ (5/50) /sooyaaa__/p/DH_AVl6J4-3/ ‚Äî 2025-04-03\n",
      "‚úÖ (6/50) /alo/p/DH7SrJJPyCx/ ‚Äî 2025-04-01\n",
      "‚úÖ (7/50) /sooyaaa__/p/DH1Ch34zp_-/ ‚Äî 2025-03-30\n",
      "‚úÖ (8/50) /sooyaaa__/p/DHyhk84JaDA/ ‚Äî 2025-03-29\n",
      "‚úÖ (9/50) /sooyaaa__/p/DHmcWcxTBT7/ ‚Äî 2025-03-24\n",
      "‚úÖ (10/50) /sooyaaa__/p/DHgcRhEJA60/ ‚Äî 2025-03-22\n",
      "‚úÖ (11/50) /sooyaaa__/p/DHdTNv_pmJN/ ‚Äî 2025-03-21\n",
      "‚úÖ (12/50) /sooyaaa__/p/DHcRRUdTUV2/ ‚Äî 2025-03-20\n",
      "‚úÖ (13/50) /sooyaaa__/p/DHaxuCqpD65/ ‚Äî 2025-03-20\n",
      "‚úÖ (14/50) /sooyaaa__/p/DHSgMKApYAv/ ‚Äî 2025-03-16\n",
      "‚úÖ (15/50) /sooyaaa__/p/DHQULHOp-x7/ ‚Äî 2025-03-16\n",
      "‚úÖ (16/50) /sooyaaa__/p/DHD9vRDpf4Y/ ‚Äî 2025-03-11\n",
      "‚úÖ (17/50) /sooyaaa__/p/DG_NfVfzFyh/ ‚Äî 2025-03-09\n",
      "‚úÖ (18/50) /sooyaaa__/p/DG7Zq1HJaSk/ ‚Äî 2025-03-07\n",
      "‚úÖ (19/50) /sooyaaa__/p/DG2oC48Jest/ ‚Äî 2025-03-06\n",
      "‚úÖ (20/50) /sooyaaa__/p/DG2hGouJm1Z/ ‚Äî 2025-03-06\n",
      "‚úÖ (21/50) /cartier/p/DG0unqrIpNv/ ‚Äî 2025-03-05\n",
      "‚úÖ (22/50) /sooyaaa__/p/DGzuzS_pCW_/ ‚Äî 2025-03-04\n",
      "‚úÖ (23/50) /sooyaaa__/p/DGy7LnytDZn/ ‚Äî 2025-03-04\n",
      "‚úÖ (24/50) /sooyaaa__/p/DGtBxDdzPob/ ‚Äî 2025-03-02\n",
      "‚úÖ (25/50) /sooyaaa__/p/DGkZcPrp0uc/ ‚Äî 2025-02-26\n",
      "‚úÖ (26/50) /sooyaaa__/p/DGh35rvpATm/ ‚Äî 2025-02-26\n",
      "‚úÖ (27/50) /sooyaaa__/p/DGaJKtnJjo_/ ‚Äî 2025-02-23\n",
      "‚úÖ (28/50) /sooyaaa__/p/DGScwLepjZ2/ ‚Äî 2025-02-20\n",
      "‚úÖ (29/50) /sooyaaa__/p/DGPxL7npnQq/ ‚Äî 2025-02-18\n",
      "‚úÖ (30/50) /sooyaaa__/p/DGMolUfPa7u/ ‚Äî 2025-02-17\n",
      "‚úÖ (31/50) /sooyaaa__/p/DGIlrsppJGI/ ‚Äî 2025-02-16\n",
      "‚úÖ (32/50) /sooyaaa__/p/DGIgzXJpSaD/ ‚Äî 2025-02-16\n",
      "‚úÖ (33/50) /sooyaaa__/p/DGF60Rjpx5S/ ‚Äî 2025-02-15\n",
      "‚úÖ (34/50) /sooyaaa__/p/DGDscKEvWnj/ ‚Äî 2025-02-14\n",
      "‚úÖ (35/50) /sooyaaa__/p/DGBICQkp4uF/ ‚Äî 2025-02-13\n",
      "‚úÖ (36/50) /sooyaaa__/p/DF-iBB8puSF/ ‚Äî 2025-02-12\n",
      "‚úÖ (37/50) /sooyaaa__/p/DFzI39PpPWp/ ‚Äî 2025-02-07\n",
      "‚úÖ (38/50) /sooyaaa__/p/DFxL06Cp_x4/ ‚Äî 2025-02-07\n",
      "‚úÖ (39/50) /sooyaaa__/p/DFvIGSQTapp/ ‚Äî 2025-02-06\n",
      "‚úÖ (40/50) /sooyaaa__/p/DFsh-pdzeig/ ‚Äî 2025-02-05\n",
      "‚úÖ (41/50) /sooyaaa__/p/DFp8wclPR9x/ ‚Äî 2025-02-04\n",
      "‚úÖ (42/50) /sooyaaa__/p/DFotZOMJIr9/ ‚Äî 2025-02-03\n",
      "‚úÖ (43/50) /sooyaaa__/p/DFnXQ6kPH-X/ ‚Äî 2025-02-03\n",
      "‚úÖ (44/50) /sooyaaa__/p/DFnW6JUPVbO/ ‚Äî 2025-02-03\n",
      "‚úÖ (45/50) /sooyaaa__/p/DFiNPf2PKHt/ ‚Äî 2025-02-01\n",
      "‚úÖ (46/50) /sooyaaa__/p/DFfpBPgTCVH/ ‚Äî 2025-01-31\n",
      "‚úÖ (47/50) /sooyaaa__/p/DFdDtFKTzFe/ ‚Äî 2025-01-30\n",
      "‚úÖ (48/50) /sooyaaa__/p/DFVzhFBzVFL/ ‚Äî 2025-01-27\n",
      "‚úÖ (49/50) /sooyaaa__/p/DFVpvNgzwA3/ ‚Äî 2025-01-27\n",
      "‚úÖ (50/50) /sooyaaa__/p/DFSxDyCvKSa/ ‚Äî 2025-01-26\n",
      "\n",
      "‚úÖ All scraping complete. Results saved to all_instagram_data.json\n"
     ]
    }
   ],
   "source": [
    "await scrape_users_from_csv(\"usernames.csv\", max_posts_per_user=50, output_json=\"all_instagram_data.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
